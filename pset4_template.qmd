---
title: "Your Title"
format: 
  pdf:
    keep-tex: true
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
---

**PS4:** Due Sat Nov 2 at 5:00PM Central. Worth 100 points. 
We use (`*`) to indicate a problem that we think might be time consuming. 
    
## Style Points (10 pts) 
Please refer to the minilesson on code style
**[here](https://uchicago.zoom.us/rec/share/pG_wQ-pHTQrJTmqNn4rcrw5V194M2H2s-2jdy8oVhWHkd_yZt9o162IWurpA-fxU.BIQlSgZLRYctvzp-)**.

## Submission Steps (10 pts)
1. This problem set is a paired problem set.
2. Play paper, scissors, rock to determine who goes first. Call that person *Partner 1*.
    - Partner 1 (name and cnet ID):
    - Partner 2 (name and cnet ID):
3. Partner 1 will accept the `ps4` and then share the link it creates with their partner. You can only share it with one partner so you will not be able to change it after your partner has accepted. 
4. "This submission is our work alone and complies with the 30538 integrity policy." Add your initials to indicate your agreement: \*\*\_\_\*\* \*\*\_\_\*\*
5. "I have uploaded the names of anyone else other than my partner and I worked with on the problem set **[here](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**"  (1 point)
6. Late coins used this pset: \*\*\_\_\*\* Late coins left after submission: \*\*\_\_\*\*
7. Knit your `ps4.qmd` to an PDF file to make `ps4.pdf`, 
    * The PDF should not be more than 25 pages. Use `head()` and re-size figures when appropriate. 
8. (Partner 1): push  `ps4.qmd` and `ps4.pdf` to your github repo.
9. (Partner 1): submit `ps4.pdf` via Gradescope. Add your partner on Gradescope.
10. (Partner 1): tag your submission in Gradescope

**Important:** Repositories are for tracking code. **Do not commit the data or shapefiles to your repo.** The best way to do this is with `.gitignore`, which we have covered in class. If you do accidentally commit the data, Github has a [guide](https://docs.github.com/en/repositories/working-with-files/managing-large-files/about-large-files-on-github#removing-files-from-a-repositorys-history). The best course of action depends on whether you have pushed yet. This also means that both partners will have to download the initial raw data and any data cleaning code will need to be re-run on both partners' computers. 

## Download and explore the Provider of Services (POS) file (10 pts)

1. 
For this exercise, I reviewed the problem set and data dictionary to determine the essential variables needed 
from the `Provider of Services (POS)` dataset. Since we are focused on analyzing short-term hospitals, 
the selected variables include:

- **Provider Category Code (PRVDR_CTGRY_CD)**: Identifies the main category of the provider (e.g., hospital).
- **Provider Category Subtype Code (PRVDR_CTGRY_SBTYP_CD)**: Further categorizes providers, with code "01" used to filter short-term hospitals.
- **State Code (STATE_CD)**: Provides the state in which the provider is located, enabling regional analysis.
- **County Code (SSA_CNTY_CD)**: Indicates the county, for more detailed geographic filtering if needed.
- **Provider Number (PRVDR_NUM)**: A unique identification number for each provider, which allows for individual tracking.
- **Certification Date (CRTFCTN_DT)**: Records the date of certification, which could be used for analysis based on the certification timeline.
- **Termination Code (PGM_TRMNTN_CD)**: Shows whether the provider is still active, helping exclude terminated providers if necessary.
- **Facility Name (FAC_NAME)**: The name of the hospital, useful for identification.
- **City Name (CITY_NAME)**: The city where the provider is located, allowing for finer geographical analysis.
- **Zip code (CITY_NAME)**:ZIP Code (ZIP_CD): The five-digit ZIP code representing the provider's physical location, useful for precise regional analysis and mapping.

2. 

```{python}
import pandas as pd
file_path = r"C:\Users\Pei-Chin\Desktop\新增資料夾\hi Dropbox\Lu Pei-Chin\pos2016.csv.csv"
data = pd.read_csv(file_path)

```
    a.
    ```{python}
    # Filter for short-term hospitals with provider category code (PRVDR_CTGRY_CD) = 1 and subtype code (PRVDR_CTGRY_SBTYP_CD) = 1
      short_term_hospitals = data[(data['PRVDR_CTGRY_CD'] == 1) & (data['PRVDR_CTGRY_SBTYP_CD'] == 1)]
    # Count the number of short-term hospitals
    hospital_count = short_term_hospitals.shape[0]
    print("Number of short-term hospitals:", hospital_count)
    # Check the number of short-term hospitals
    hospital_count = short_term_hospitals.shape[0]
    print("Number of short-term hospitals:", hospital_count)
    ```
    b.
    ##Based on my analysis, I identified **7,245** short-term hospitals in the 2016 dataset, using `provider 
    ##type code = 01` and `subtype code = 01`. This count is notably higher than external estimates, 
    ##which report approximately **4,862** short-term acute care hospitals in 2016.

    ##The discrepancy may be due to differences in dataset definitions or the inclusion of additional facility 
    ##types in the `pos2016.csv` file. Further examination is required to align the dataset criteria with 
    ##standard definitions used in external sources.

    ##Reference:
    ##- Agency for Healthcare Research and Quality (AHRQ)


3.
```{python}
import pandas as pd
import matplotlib.pyplot as plt
import os

# Define file paths for each year
file_paths = {
    '2016': r'C:\Users\Pei-Chin\Desktop\新增資料夾\hi Dropbox\Lu Pei-Chin\pos2016.csv.csv',
    '2017': r'C:\Users\Pei-Chin\Desktop\新增資料夾\hi Dropbox\Lu Pei-Chin\pos2017.csv.csv',
    '2018': r'C:\Users\Pei-Chin\Desktop\新增資料夾\hi Dropbox\Lu Pei-Chin\pos2018.csv.csv',
    '2019': r'C:\Users\Pei-Chin\Desktop\新增資料夾\hi Dropbox\Lu Pei-Chin\pos2019.csv.csv'
}

# Dictionary to store filtered data for each year
data = {}

# Loop through each file and check if it exists
for year, file_path in file_paths.items():
    if os.path.exists(file_path):
        try:
            # Load data with specified encoding
            df = pd.read_csv(file_path, encoding="ISO-8859-1", low_memory=False)
            
            # Filter for short-term hospitals: provider category code = 1 and subtype code = 1
            df_filtered = df[(df['PRVDR_CTGRY_CD'] == 1) & (df['PRVDR_CTGRY_SBTYP_CD'] == 1)].copy()
            df_filtered['Year'] = year  # Add year column
            
            # Store filtered data for each year
            data[year] = df_filtered
            print(f"Data for {year} loaded and filtered successfully.")
            
        except Exception as e:
            print(f"Error loading {file_path}: {e}")
    else:
        print(f"File for {year} does not exist: {file_path}")
```

# Check if any data was loaded
```{python}
if data:
    # Combine all years into one DataFrame
    all_data = pd.concat(data.values(), ignore_index=True)

    # Question 3: Plot the number of observations by year
    observations_by_year = all_data['Year'].value_counts().sort_index()
    observations_by_year.plot(kind='bar', title="Number of Observations by Year")
    plt.xlabel('Year')
    plt.ylabel('Number of Observations')
    plt.show()
```
4.
```{python}
    # Question 4(a): Plot the number of unique hospitals by year (using 'PRVDR_NUM' as a unique identifier)
    unique_hospitals_by_year = all_data.groupby('Year')['PRVDR_NUM'].nunique()
    unique_hospitals_by_year.plot(kind='bar', color='orange', title="Number of Unique Hospitals by Year")
    plt.xlabel('Year')
    plt.ylabel('Number of Unique Hospitals')
    plt.show()

```

```{python}
    # Question 4(b): Verification that each hospital appears only once per year
    # Count the occurrences of each hospital per year
    hospital_counts = all_data.groupby(['Year', 'PRVDR_NUM']).size()


    # Find cases where a hospital appears more than once in a given year
    duplicates = hospital_counts[hospital_counts > 1]
    
    # Check for duplicates
    if duplicates.empty:
        print("Each hospital appears only once per year. Data structure is valid.")
    else:
        print("Some hospitals appear multiple times per year. Below are the details:")
        print(duplicates)

```

## Identify hospital closures in POS file (15 pts) (*)

1. 
2. 
3. 
    a.
    b.
    c.

## Download Census zip code shapefile (10 pt) 

1. 
    a.The five file types found in the downloaded dataset are .shp, .shx, .dbf, .prj, and .xml. Each 
    type serves a specific purpose: .shp stores the geometrical shapes of geographical features, .shx acts 
    as an index file for faster access, .dbf contains attribute data related to each feature, .prj defines 
    the projection and coordinate system, and .xml holds metadata describing the dataset.

    b.After unzipping the file, the sizes of each file type are as follows: the .shp file is approximately 
    514 MB, the .dbf file is about 6 MB, the .shx file is 259 KB, the .prj file is 1 KB, and the .xml file 
    is 16 KB. This indicates that the .shp file holds the most data as it stores the detailed geometrical 
    information of geographical features.
2. 

```{python}
import geopandas as gpd
import pandas as pd
import matplotlib.pyplot as plt

# Load the ZIP code shapefile
shapefile_path = r"C:\Users\Pei-Chin\Desktop\新增資料夾\hi Dropbox\Lu Pei-Chin\gz_2010_us_860_00_500k\gz_2010_us_860_00_500k.shp"
zip_gdf = gpd.read_file(shapefile_path)

# Filter for Texas ZIP codes (starting with '75' or '77')
texas_zip_gdf = zip_gdf[zip_gdf['ZCTA5'].astype(str).str.startswith(('75', '77'))]

# Load the cleaned POS hospital data for 2016
pos_data_path = r"C:\Users\Pei-Chin\Desktop\新增資料夾\hi Dropbox\Lu Pei-Chin\pos2016.csv.csv"
pos_df = pd.read_csv(pos_data_path, dtype={'ZIP_CD': str}, low_memory=False)

# Filter POS data for Texas ZIP codes (using ZIP_CD)
texas_pos_df = pos_df[pos_df['ZIP_CD'].astype(str).str.startswith(('75', '77'))]

# Calculate the number of hospitals per ZIP code in Texas
hospital_counts = texas_pos_df['ZIP_CD'].value_counts().reset_index()
hospital_counts.columns = ['ZIP_CD', 'Hospital_Count']

# Merge with Texas ZIP shapefile data
texas_zip_gdf = texas_zip_gdf.merge(hospital_counts, left_on='ZCTA5', right_on='ZIP_CD', how='left').fillna(0)

# Plot choropleth of the number of hospitals by ZIP code in Texas
fig, ax = plt.subplots(1, 1, figsize=(10, 10))
texas_zip_gdf.plot(column='Hospital_Count', cmap='OrRd', linewidth=0.8, ax=ax, edgecolor='0.8', legend=True)
plt.title("Number of Hospitals by ZIP Code in Texas (2016)")
plt.xlabel("Longitude")
plt.ylabel("Latitude")
plt.show()
```


## Calculate zip code’s distance to the nearest hospital (20 pts) (*)

1. 
2. 
3. 
4. 
    a.
    b.
5. 
    a.
    b.
    c.
    
## Effects of closures on access in Texas (15 pts)

1. 
2. 
3. 
4. 

## Reflecting on the exercise (10 pts) 
